{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itC6u5MwXTIu"
   },
   "source": [
    "# Steam Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Some apps have different steam_appids from the ones we are downloading them from - need a check for that since SteamSpy might have different appid from the storefront in that case\n",
    "* Check the data update: add different options for full/partial load. Check timestamp on the lastupdate\n",
    "* Automating data gathering (separating into different scripts to run in parallel?)\n",
    "* Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tB7Ot-2XTI2",
    "tags": []
   },
   "source": [
    "## Project Goals\n",
    "\n",
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "\n",
    "The motivation is gather, process and analyze Steam Store data to get insights about trends in the videogame market. As it is an online marketplace with public available data, it offers us more possibilities than analyzing console games data, where we would have to rely on an existing dataset.\n",
    "\n",
    "We want to focus on two main aspects, first a general market analysis to know which genres are the most popular, pricing strategies and so on, which could be interesting for a new developer trying to make a new game or deciding a price policy. This has been studied already by other enthusiasts in internet, and also by Marketing companies helping publishers.\n",
    "\n",
    "But to offer a different analysis, we want to also focus on the developers and publishers, to see which ones are the most successfull, how they have improved / worsen between the years, which titles have cemented their success and so on. In the light of recent years we have seen many acquisitions by large publishers such as Tencent, Microsoft and Sony, so this is very interesting concept.\n",
    "\n",
    "This will be a complete data project, with a data acquisition section (by using some APIs and web scrapping), then data cleaning and joining data from different sources, an exploratory data analysis, and finally some key conclusions.\n",
    "\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "This is the section where I struggled initially. There were several datasets available at [kaggle](https://www.kaggle.com/datasets), reddit and similar websites, but most were outdated or did not contain all the information I wanted to explore. Also I wanted to extract it directly from an API or use web scrapping, if possible, to learn a bit more (I already had experience with Twitter which has an excellent API).\n",
    "\n",
    "[SteamSpy](https://steamspy.com/about) is a webpage which offers data about Steam games. In the past it was even able to deliver a good guess of sales, but that has become harder throughout the years. Check [VG Insights](https://vginsights.com/insights/article/how-to-estimate-steam-video-game-sales) for more information. It is also a good webpage if you want to explore market data on your own.\n",
    "The most important thing for Steamspy is that it has its own API [here](https://steamspy.com/api.php). It can provide us easily with an already filtered list of games (not other apps or DLCs), and also some metrics not available at Steam directly such as an estimate of sales and the positive or negative reviews (Steam only gives us total number reviews).\n",
    "\n",
    "Regarding Steam directly, the API is available at https://partner.steamgames.com/ , however you need a developer key and some (most of the functions) are tied to your key as they are intended to be used to manage your own products at the Steam store. Thanks to [Nik Davis](http://nik-davis.github.io) I discovered there were also a few API functions via the WEB API which can be used without a key at all. See here for more details: [StorefrontAPI](https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI).\n",
    "\n",
    "Getting the information from Steam will be a bit more difficult, but it will give us additional metrics, such as release date, genre...\n",
    "\n",
    "We will retrieve first the list of appids and the information available at Steam Spy, then get for each appid the information from Steam and combine them in an unique dataframe. There will be no loss of information as app ids are unique. Afterwards, we will perform cleaning and finally start analyzing our dataset.\n",
    "\n",
    "## Process:\n",
    "\n",
    "- Create an app list and gather available data from SteamSpy API using 'all' request\n",
    "- Retrieve individual app data from Steam API, by iterating through app list\n",
    "- Export app list, Steam data and SteamSpy data to csv files\n",
    "\n",
    "## API references:\n",
    "\n",
    "- https://partner.steamgames.com/doc/webapi\n",
    "- https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI\n",
    "- https://steamapi.xpaw.me/#\n",
    "- https://steamspy.com/api.php\n",
    "\n",
    "\n",
    "## Credits\n",
    "\n",
    "The most important source I found while looking how to connect to the API was Nik Davis, check his blog for a different analysis on steam data (from 2019) http://nik-davis.github.io\n",
    "Download functions for the APIs are based on his notebook for \"Steam Data Download\". I had to make some changes and simplify a bit.\n",
    "\n",
    "Steamspy seems to have changed its API, so I had to change the download method to instead download all the data by page (set of 1000 ids). The functions defined for Steam API itself still work as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cB_ucVTyXTI3"
   },
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests.auth\n",
    "\n",
    "# customisations - ensure tables show all columns\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up proxies and Steam API key:\n",
    "try:\n",
    "    with open('../data/_credentials/steam_key.txt') as f:\n",
    "        APIKey = f.read()\n",
    "except:\n",
    "    APIKey = \"\"\n",
    "    \n",
    "try:\n",
    "    with open('../data/_credentials/proxies.txt', 'r') as f:\n",
    "        proxies = eval(f.read())\n",
    "except:\n",
    "    proxies = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function uses requests library to get JSON response from web APIs. It is based on Nik Davis previous work, and it is quite standard as (thankfully) web APIs use a standard format, and requests makes it really easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hggyD3aNXTI5"
   },
   "outputs": [],
   "source": [
    "def get_request(url,parameters=None, steamspy=False):\n",
    "    \"\"\"Return json-formatted response of a get request using optional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "        parameters to pass as part of get request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        response = requests.get(url=url, params=parameters, headers = headers, proxies = proxies)\n",
    "    except requests.exceptions.SSLError as s:\n",
    "        print('SSL Error:', s)\n",
    "        \n",
    "        for i in range(5, 0, -1):\n",
    "            print('\\rWaiting... ({})'.format(i), end='')\n",
    "            time.sleep(1)\n",
    "        print('\\rRetrying.' + ' '*10)\n",
    "        \n",
    "        # recursively try again\n",
    "        return get_request(url, parameters, steamspy)\n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # We do not know how many pages steamspy has... and it seems to work well, so we will use no response to stop.\n",
    "        if steamspy:\n",
    "            return \"stop\"\n",
    "        else :\n",
    "            # response is none usually means too many requests. Wait and try again \n",
    "            print('No response, waiting 15 seconds...')\n",
    "            time.sleep(15)\n",
    "            print('Retrying.')\n",
    "            return get_request(url, parameters, steamspy)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfR35UFwXTI5"
   },
   "source": [
    "## List of IDs\n",
    "\n",
    "APPs on steam have an unique ID. The requests to Steam API (which has more information than Steam Spy) have to be made for a specific ID. This means we have to get first a list of ids.\n",
    "\n",
    "We can do this in several ways, but this is what I decided to follow:\n",
    "\n",
    "* Using Steam Spy API (see https://steamspy.com/api.php) to get the list of IDs and also the metadata from Steam Spy (at the same time. Unfortunately, using this method gives a lot of duplicates and headaches.\n",
    "\n",
    "\n",
    "* Alternatively, we could use Steam API to get a list of apps, then filter them (see https://api.steampowered.com/ISteamApps/GetAppList/v1/? or https://steamapi.xpaw.me/#IStoreService/GetAppInfo)\n",
    "\n",
    "We will use the Steam GetAppList API and use the list of apps from it to index across all the tables we use eventually for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx8MClSGXTI6"
   },
   "source": [
    "## Define Download Logic\n",
    "\n",
    "This is strongly based on Nik Davis previous work, to get the info about the app IDs from Steam. Initially I prefered to focus on the analysis rather than in the acquisition phase, but I watn to modify these functions to change them from an index based approach for the update, to instead check already existing ids in the database and download only the delta.\n",
    "\n",
    "Later, we could maybe also add a check to see if the app has been updated or not and even redownload the info from not just new IDs, but IDs that have been updated.\n",
    "\n",
    "I will keep the original comments from Nik Davis `in quotes` to let the reader understand the process.\n",
    "\n",
    "`Now we have the app_list dataframe, we can iterate over the app IDs and request individual app data from the servers. Here we set out our logic to retrieve and process this information, then finally store the data as a csv file.`\n",
    "\n",
    "`Because it takes a long time to retrieve the data, it would be dangerous to attempt it all in one go as any errors or connection time-outs could cause the loss of all our data. For this reason we define a function to download and process the requests in batches, appending each batch to an external file and keeping track of the highest index written in a separate file.`\n",
    "\n",
    "`This not only provides security, allowing us to easily restart the process if an error is encountered, but also means we can complete the download across multiple sessions.`\n",
    "\n",
    "`Again, we provide verbose output for rows exported, batches complete, time taken and estimated time remaining.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1eLPdKoGXTI6"
   },
   "outputs": [],
   "source": [
    "def get_app_data(app_list, start, stop, parser, pause, errors_list):\n",
    "    \"\"\"Return list of app data generated from parser.\n",
    "    \n",
    "    parser : function to handle request\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, appid in app_list[start:stop].iteritems():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "\n",
    "        # retrive app data for a row, handled by supplied parser, and append to list\n",
    "        try:\n",
    "            data = parser(appid)\n",
    "        except Exception as ex:\n",
    "            errors_list.append(appid)\n",
    "            print('\\nError getting data for {} with exception {}\\n'.format(appid, type(ex).__name__))\n",
    "        app_data.append(data)\n",
    "\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data\n",
    "\n",
    "\n",
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    errors_list, columns,\n",
    "                    begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    errors_list : list to store appid errors\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(app_list, start, stop, parser, pause, errors_list)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to use this function and still only get the newer apps, would be to instead of passing it the fully app_list, preprocess it so it only contains the \"app_delta\". Also we would need it to keep the final dataframe in a different file, to perform after an append to it (in case we are adding only new app_ids), or if we add some kind of updating process, a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP2UaFpyXTI7"
   },
   "source": [
    "`Next we define some functions to handle and prepare the external files.`\n",
    "\n",
    "`We use reset_index for testing and demonstration, allowing us to easily reset the index in the stored file to 0, effectively restarting the entire download process.`\n",
    "\n",
    "`We define get_index to retrieve the index from file, maintaining persistence across sessions. Every time a batch of information (app data) is written to file, we write the highest index within app_data that was retrieved. As stated, this is partially for security, ensuring that if there is an error during the download we can read the index from file and continue from the end of the last successful batch. Keeping track of the index also allows us to pause the download, continuing at a later time.`\n",
    "\n",
    "`Finally, the prepare_data_file function readies the csv for storing the data. If the index we retrieved is 0, it means we are either starting for the first time or starting over. In either case, we want a blank csv file with only the header row to begin writing to, se we wipe the file (by opening in write mode) and write the header. Conversely, if the index is anything other than 0, it means we already have downloaded information, and can leave the csv file alone.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "79z_mTOoXTI7"
   },
   "outputs": [],
   "source": [
    "def reset_index(download_path, index_filename):\n",
    "    \"\"\"Reset index in file to 0.\"\"\"\n",
    "    rel_path = os.path.join(download_path, index_filename)\n",
    "    \n",
    "    f= open(rel_path, 'w')\n",
    "    f.write(\"0\")\n",
    "        \n",
    "\n",
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "            #This just reads the initial line\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index\n",
    "\n",
    "\n",
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnD4UGAnXTI7"
   },
   "source": [
    "## Download Steam Data\n",
    "\n",
    "`Now we are ready to start downloading data and writing to file. We define our logic particular to handling the steam API - in fact if no data is returned we return just the name and appid - then begin setting some parameters. We define the files we will write our data and index to, and the columns for the csv file. The API doesn't return every column for every app, so it is best to explicitly set these.`\n",
    "\n",
    "`Next we run our functions to set up the files, and make a call to process_batches to begin the process. Some additional parameters have been added for demonstration, to constrain the download to just a few rows and smaller batches. Removing these would allow the entire download process to be repeated.`\n",
    "\n",
    "I retouched many of these parameters just to check if the download could made in batches (requesting several steamapps at the same time), or even putting a faster polling rate (right now it is one second).\n",
    "\n",
    "The storefront API (http://store.steampowered.com/api/) is very much undocumented, but the key getaway is that is not possible. The official SteamWorks API lets us do other things and it is quite well documented, but we cannot get the data available at a steam webpage, which are the things interesting for us.\n",
    "\n",
    "The storefront API is only accessible with these [requests](https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI), and according to this [stackoverflow discussion](https://stackoverflow.com/questions/46330864/steam-api-all-games):\n",
    "`There is a general API rate limit for each unique IP adress of 200 requests in five minutes which is one request every 1.5 seconds.` This matches our experience, Nik Davis put a pause between requests of just 1 second, and with this we get some but a few reconnect errors. If we put no pause at all, at the end we are limited by the 200 requests every 5 minutes.\n",
    "\n",
    "That means that for a volume of around 50k at January 2022 (the steam apps available also at steam spy, already filtered by game and some owner data...) this download will take around 21 hours. Thankfully we can resume it and do it in several batches.\n",
    "\n",
    "If we were to build a web app and we wanted to update the information daily, we could instead try pulling the full applist from steam along with the \"last updated\" and only request the full appid information for those ids. This is probably what Steam Spy does, and SteamDB instead uses a more sofisticated approach by being notified of any changes to appids via steamworks.\n",
    "\n",
    "In any case, for a one shot analysis (and not a web page where the user could explore the information), the full download approach is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the next two functions is getting the full list of apps from Steam, and also getting from our already downloaded data (if any) which are the appids we already have, to get the \"delta\" so we only have to download the new app ids.\n",
    "\n",
    "We could retouch them a bit, as they have a \"last updated\" key, so we also update new information and not just new ids. But that would be more suitable for a live webpage updated once a day, not for a full on analysis like we are presenting.\n",
    "*NOTE:* Added 'include_dlc' key to include game DLCs for additional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppListBatch(url, parameters):\n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    steam_id = pd.DataFrame.from_dict(json_data[\"response\"][\"apps\"])\n",
    "    try:\n",
    "        more_results = json_data[\"response\"][\"have_more_results\"]\n",
    "        last_appid =  json_data[\"response\"][\"last_appid\"]\n",
    "    except:\n",
    "        more_results = False\n",
    "        last_appid = False\n",
    "    return more_results, steam_id, last_appid\n",
    "\n",
    "def get_update_ids_old(updatedlist, oldlist):\n",
    "    updatedlist['key1'] = 1\n",
    "    oldlist['key2'] = 1\n",
    "    updatedlist = pd.merge(updatedlist, oldlist, right_on=['steam_appid','name'],left_on=['appid','name'], how = 'outer')\n",
    "    updatedlist = updatedlist[~(updatedlist.key2 == updatedlist.key1)]\n",
    "    updatedlist = updatedlist.drop(['key1','key2','steam_appid'], axis=1)\n",
    "    return updatedlist\n",
    "\n",
    "def get_update_ids(idList, oldFullList):\n",
    "    #We are going to forget about names and only care about IDs.\n",
    "    idList = idList[\"appid\"]\n",
    "    oldFullList = oldFullList[\"steam_appid\"]\n",
    "    oldFullList.columns = [\"appid\"]\n",
    "    updatedList = idList.append(oldFullList)\n",
    "    updatedList = updatedList.drop_duplicates(keep=False)\n",
    "    updatedList = updatedList.reset_index(drop=True)\n",
    "    return updatedList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppList():\n",
    "\n",
    "    url = \"https://api.steampowered.com/IStoreService/GetAppList/v1/?\"\n",
    "    parameters = {\"key\": APIKey,\n",
    "                 \"include_dlc\": \"true\"}\n",
    "    more_results = True\n",
    "    begin = True\n",
    "    # from the request we get the more_results flag and also the last_appid, so we use them for the next requests.\n",
    "    while (more_results):\n",
    "        more_results, steam_ids, last_appid = getAppListBatch(url, parameters)\n",
    "        parameters[\"last_appid\"] = last_appid\n",
    "        if (begin):\n",
    "            steam_allids = steam_ids\n",
    "            begin = False\n",
    "        else:\n",
    "            steam_allids = steam_allids.append(steam_ids)\n",
    "    return steam_allids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "W7e3eUD2XTI8",
    "outputId": "3dced899-ac96-42b4-e779-2e7cf68c4234",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_steam_request(appid):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "    parameters = {\"appids\": appid, \"key\": APIKey}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'steam_appid': appid}\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Set file parameters\n",
    "download_path = '../data/download/'\n",
    "steam_app_data = 'steam_app_data.csv'\n",
    "steam_app_data_delta = 'steam_app_data_delta.csv'\n",
    "steam_index = 'steam_index.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n",
    "\n",
    "steam_errors = []\n",
    "\n",
    "# Overwrites last index for demonstration (would usually store highest index so can continue across sessions)\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    reset_index(download_path, steam_index)\n",
    "\n",
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "\n",
    "# Wipe or create data file and write headers if no previous  data\n",
    "if (os.path.isfile(download_path+steam_app_data) == False):\n",
    "    prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "    \n",
    "# Wipe or create data file delta and write headers if index is 0\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    prepare_data_file(download_path, steam_app_data_delta, index, steam_columns)\n",
    "    \n",
    "    \n",
    "# Here we get the list of appids from steam\n",
    "full_steam_ids = getAppList()\n",
    "\n",
    "# Here we get the real list of ids not yet in our dataframe. If this is the first time we are downloading the data, we can skip\n",
    "# This step and instead use the full app_list.\n",
    "try:\n",
    "    oldlist = pd.read_csv('../data/download/steam_app_data.csv', usecols = ['name','steam_appid'])\n",
    "    steam_ids = get_update_ids(full_steam_ids, oldlist)\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-existing file not found. First time downloading full app data from steam. This will take a while.\\n\")\n",
    "    steam_ids = full_steam_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New IDs detected: 103145\n",
      "0              10\n",
      "1              20\n",
      "2              30\n",
      "3              40\n",
      "4              50\n",
      "           ...   \n",
      "103140    2028023\n",
      "103141    2028055\n",
      "103142    2028056\n",
      "103143    2028062\n",
      "103144    2028850\n",
      "Length: 103145, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"New IDs detected: \"+str(len(steam_ids)))\n",
    "print(steam_ids)\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I separated the long process to be able to debug it better.\n",
    "# Set end and chunksize for demonstration - remove to run through entire app list\n",
    "# Here by default we passed \"app_list\" that contained all the information and saved it, now we will modify it a bit\n",
    "# And add pre-processing and post-processing\n",
    "print(\"Adding \"+str(len(steam_ids))+\" new ids.\\n\")\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=steam_ids,\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data_delta,\n",
    "    index_filename=steam_index,\n",
    "    errors_list=steam_errors,\n",
    "    columns=steam_columns,\n",
    "    begin=index,\n",
    "    #end=10,\n",
    "    #pause=0.5\n",
    "    batchsize=100,\n",
    "    pause=1\n",
    ")\n",
    "\n",
    "try:\n",
    "    oldlist = pd.read_csv('../data/download/steam_app_data.csv')\n",
    "    # We change the old file to backup, so remove any backup named this way before...\n",
    "    os.replace('../data/download/steam_app_data.csv', '../data/download/steam_app_data_backup.csv')\n",
    "    newlist = pd.read_csv('../data/download/steam_app_data_delta.csv')\n",
    "    oldlist = oldlist.append(newlist, ignore_index=True)\n",
    "    oldlist.to_csv('../data/download/steam_app_data.csv', index=False)\n",
    "    steam_errors_df = pd.DataFrame(steam_errors, columns=[\"appid\"])\n",
    "    steam_errors_df.to_csv('../data/download/steam_errors.csv', index=False)\n",
    "except FileNotFoundError:\n",
    "    os.rename('../data/download/steam_app_data_delta.csv', '../data/download/steam_app_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we have no duplicate ids and that we got them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data = pd.read_csv('../data/download/steam_app_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_app_data.duplicated(subset=\"steam_appid\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some duplicates here. Let's compare that to the full set of ids.\n",
    "(And also save the current full Steam ids for the future reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_steam_ids1 = getAppList()\n",
    "full_steam_ids_df = pd.DataFrame(full_steam_ids1, columns =['appid'])\n",
    "full_steam_ids_df.to_csv(\"../data/download/full_steam_ids.csv\", index=False)\n",
    "full_steam_ids1.duplicated(subset=\"appid\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we cleaned before, it is possible we got some ids twice. Let's compare the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(steam_app_data)-len(full_steam_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we were using old data, it seems like ids that are no longer available are still there, along with a few duplicates. Let's run again the function which would get new ids, just to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ids = get_update_ids(full_steam_ids, steam_app_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diff_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 14 new ids. Taking into account that around a hundred apps get uploaded to Steam everyday, this makes sense, so we do not need to download anything new for the moment, just cleaning.\n",
    "\n",
    "Well, from Steam... now we have to make sure we got most of these IDs from SteamSpy as well.\n",
    "\n",
    "Let's do a bit of pre-cleaning, to ensure we download only the ids we need from Steam Spy.\n",
    "\n",
    "We are going to consider valid apps those that at least have a name for the moment. Then delete the remaining duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data = steam_app_data.drop_duplicates(subset=\"steam_appid\", keep=\"last\")\n",
    "steam_app_data.to_csv(\"../data/download/steam_app_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appids that were not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>281341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>712350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1072170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1163550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1215750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1262349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1354970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1389830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1440670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1444140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1472160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1671760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1765730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1863540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      appid\n",
       "0    281341\n",
       "1    712350\n",
       "2   1023100\n",
       "3   1061400\n",
       "4   1072170\n",
       "5   1163550\n",
       "6   1215750\n",
       "7   1262349\n",
       "8   1354970\n",
       "9   1389830\n",
       "10  1440670\n",
       "11  1444140\n",
       "12  1472160\n",
       "13  1489900\n",
       "14  1671760\n",
       "15  1709200\n",
       "16  1765730\n",
       "17  1863540"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_errors_df = pd.DataFrame(steam_errors, columns =['appid'])\n",
    "steam_errors_df.to_csv(\"../data/download/steam_errors.csv\", index=False)\n",
    "steam_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HEyFNt-aXTI8",
    "outputId": "2905c74e-5f12-463f-bd46-05f0dba8ce54",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102436 entries, 0 to 103144\n",
      "Data columns (total 39 columns):\n",
      " #   Column                   Non-Null Count   Dtype \n",
      "---  ------                   --------------   ----- \n",
      " 0   type                     102436 non-null  object\n",
      " 1   name                     102436 non-null  object\n",
      " 2   steam_appid              102436 non-null  int64 \n",
      " 3   required_age             102436 non-null  object\n",
      " 4   is_free                  102436 non-null  object\n",
      " 5   controller_support       25479 non-null   object\n",
      " 6   dlc                      9693 non-null    object\n",
      " 7   detailed_description     102286 non-null  object\n",
      " 8   about_the_game           102285 non-null  object\n",
      " 9   short_description        102282 non-null  object\n",
      " 10  fullgame                 34580 non-null   object\n",
      " 11  supported_languages      102262 non-null  object\n",
      " 12  header_image             102436 non-null  object\n",
      " 13  website                  60038 non-null   object\n",
      " 14  pc_requirements          102436 non-null  object\n",
      " 15  mac_requirements         102436 non-null  object\n",
      " 16  linux_requirements       102436 non-null  object\n",
      " 17  legal_notice             40747 non-null   object\n",
      " 18  drm_notice               706 non-null     object\n",
      " 19  ext_user_account_notice  1037 non-null    object\n",
      " 20  developers               102125 non-null  object\n",
      " 21  publishers               102436 non-null  object\n",
      " 22  demos                    6504 non-null    object\n",
      " 23  price_overview           80060 non-null   object\n",
      " 24  packages                 81136 non-null   object\n",
      " 25  package_groups           102436 non-null  object\n",
      " 26  platforms                102436 non-null  object\n",
      " 27  metacritic               3824 non-null    object\n",
      " 28  reviews                  9082 non-null    object\n",
      " 29  categories               102331 non-null  object\n",
      " 30  genres                   102244 non-null  object\n",
      " 31  screenshots              102279 non-null  object\n",
      " 32  movies                   68304 non-null   object\n",
      " 33  recommendations          13183 non-null   object\n",
      " 34  achievements             31071 non-null   object\n",
      " 35  release_date             102436 non-null  object\n",
      " 36  support_info             102436 non-null  object\n",
      " 37  background               102297 non-null  object\n",
      " 38  content_descriptors      102436 non-null  object\n",
      "dtypes: int64(1), object(38)\n",
      "memory usage: 31.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# inspect downloaded data\n",
    "steam_app_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfR35UFwXTI5"
   },
   "source": [
    "## Steam Spy API\n",
    "\n",
    "APPs on steam have an unique ID. The requests to Steam API (which has more information than Steam Spy) have to be made for a specific ID. This means we have to get first a list of ids.\n",
    "\n",
    "We can do this in several ways, but this is what I decided to follow:\n",
    "\n",
    "* Using Steam Spy API (see https://steamspy.com/api.php) to get the list of IDs and also the metadata from Steam Spy (at the same time). Alternatively, we could use Steam API to get a list of apps, then filter them (see https://api.steampowered.com/ISteamApps/GetAppList/v2/? or https://steamapi.xpaw.me/#IStoreService/GetAppInfo). Unfortunately, using this method got me \n",
    "\n",
    "* Then using Steam API to loop for each ID from the list and getting the complete info.\n",
    "\n",
    "We are going to use this request: https://steamspy.com/api.php?request=all&page=1 - return apps 1,000-1,999 of all apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steamspy_request(appid):\n",
    "    \"\"\"Parser to handle SteamSpy API data.\"\"\"\n",
    "    url = \"https://steamspy.com/api.php\"\n",
    "    parameters = {\"request\": \"appdetails\", \"appid\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# set files and columns\n",
    "download_path = '../data/download'\n",
    "steamspy_data = 'steamspy_data.csv'\n",
    "steamspy_index = 'steamspy_index.txt'\n",
    "\n",
    "steamspy_columns = [\n",
    "    'appid', 'name', 'developer', 'publisher', 'score_rank', 'positive',\n",
    "    'negative', 'userscore', 'owners', 'average_forever', 'average_2weeks',\n",
    "    'median_forever', 'median_2weeks', 'price', 'initialprice', 'discount',\n",
    "    'languages', 'genre', 'ccu', 'tags'\n",
    "]\n",
    "\n",
    "steamspy_errors = []\n",
    "\n",
    "reset_index(download_path, steamspy_index)\n",
    "index = get_index(download_path, steamspy_index)\n",
    "\n",
    "# Wipe data file if index is 0\n",
    "prepare_data_file(download_path, steamspy_data, index, steamspy_columns)\n",
    "\n",
    "process_batches(\n",
    "    parser=parse_steamspy_request,\n",
    "    app_list=full_steam_ids[\"appid\"],\n",
    "    download_path=download_path, \n",
    "    data_filename=steamspy_data,\n",
    "    index_filename=steamspy_index,\n",
    "    errors_list=steamspy_errors,\n",
    "    columns=steamspy_columns,\n",
    "    begin=index,\n",
    "    end=len(full_steam_ids),\n",
    "    batchsize=300,\n",
    "    pause=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[326460, 392780, 590187, 1018960, 1018990, 1019000, 1019190, 1103750, 1705313]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamspy_errors_df = pd.DataFrame(steamspy_errors, columns =['appid'])\n",
    "steamspy_errors_df.to_csv(\"../data/download/steamspy_errors.csv\", index=False)\n",
    "steamspy_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check if we have valid data inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_spy_data = pd.read_csv('../data/download/steamspy_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103145 entries, 0 to 103144\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   appid            103145 non-null  int64  \n",
      " 1   name             102896 non-null  object \n",
      " 2   developer        92436 non-null   object \n",
      " 3   publisher        83261 non-null   object \n",
      " 4   score_rank       52 non-null      float64\n",
      " 5   positive         103145 non-null  int64  \n",
      " 6   negative         103145 non-null  int64  \n",
      " 7   userscore        103145 non-null  int64  \n",
      " 8   owners           103145 non-null  object \n",
      " 9   average_forever  103145 non-null  int64  \n",
      " 10  average_2weeks   103145 non-null  int64  \n",
      " 11  median_forever   103145 non-null  int64  \n",
      " 12  median_2weeks    103145 non-null  int64  \n",
      " 13  price            92793 non-null   float64\n",
      " 14  initialprice     92804 non-null   float64\n",
      " 15  discount         92804 non-null   float64\n",
      " 16  languages        92583 non-null   object \n",
      " 17  genre            92487 non-null   object \n",
      " 18  ccu              103145 non-null  int64  \n",
      " 19  tags             103145 non-null  object \n",
      "dtypes: float64(4), int64(9), object(7)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "steam_spy_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_spy_data.duplicated(subset=\"appid\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "Now we have the Steam Spy data available on `../data/download/steamspy_data.csv` and the Steam Store data available on `../data/download/steam_app_data.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steam Reviews\n",
    "\n",
    "While exploring why the steam reviews on steam spy and the steam store webpage itself were not the same, we got a way to check via the API partners itself. Let's try to obtain this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steamreviews_request(appid):\n",
    "    \"\"\"Parser to handle SteamSpy API data.\"\"\"\n",
    "    url = \"https://store.steampowered.com/appreviews/\" + str(appid)\n",
    "    #todo: add purchase_type=all in parameters for the next version\n",
    "    parameters = {\"json\": 1, \"num_per_page\": \"0\", \"language\": \"all\", \"purchase_type\": \"all\"}\n",
    "    json_data = get_request(url, parameters)\n",
    "    json_data = json_data['query_summary']\n",
    "    json_data[\"appid\"]=appid\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# set files and columns\n",
    "download_path = '../data/download'\n",
    "steamreviews_data = 'steamreviews_data.csv'\n",
    "steamreviews_index = 'steamreviews_index.txt'\n",
    "\n",
    "steamreviews_columns = [\n",
    "    'appid', 'review_score', 'review_score_desc', 'total_positive', 'total_negative', 'total_reviews'\n",
    "]\n",
    "\n",
    "steamreviews_errors = []\n",
    "\n",
    "#Reset index if to download the reviews from 0\n",
    "#reset_index(download_path, steamreviews_index)\n",
    "index = get_index(download_path, steamreviews_index)\n",
    "\n",
    "# Wipe data file if index is 0\n",
    "prepare_data_file(download_path, steamreviews_data, index, steamreviews_columns)\n",
    "\n",
    "full_steam_ids=pd.read_csv(\"../data/download/steam_app_data.csv\")\n",
    "\n",
    "process_batches(\n",
    "    parser=parse_steamreviews_request,\n",
    "    app_list=full_steam_ids[\"steam_appid\"],\n",
    "    download_path=download_path, \n",
    "    data_filename=steamreviews_data,\n",
    "    index_filename=steamreviews_index,\n",
    "    errors_list=steamreviews_errors,\n",
    "    columns=steamreviews_columns,\n",
    "    begin=index,\n",
    "    end=len(full_steam_ids),\n",
    "    batchsize=300,\n",
    "    pause=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "steamreviews=pd.read_csv(\"../data/download/steamreviews_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[303530, 433743, 602180, 1085460, 1508400, 1890670]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamreviews_errors_df = pd.DataFrame(steamreviews_errors, columns =['appid'])\n",
    "steamreviews_errors_df.to_csv(\"../data/download/steamreviews_errors.csv\", index=False)\n",
    "steamreviews_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102436 entries, 0 to 102435\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   appid              102436 non-null  int64  \n",
      " 1   review_score       102396 non-null  float64\n",
      " 2   review_score_desc  102396 non-null  object \n",
      " 3   total_positive     102396 non-null  float64\n",
      " 4   total_negative     102396 non-null  float64\n",
      " 5   total_reviews      102396 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "steamreviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0       37626\n",
       "1.0        7625\n",
       "2.0        5171\n",
       "3.0        3727\n",
       "4.0        3006\n",
       "          ...  \n",
       "1602.0        1\n",
       "2996.0        1\n",
       "3588.0        1\n",
       "1938.0        1\n",
       "3198.0        1\n",
       "Name: total_reviews, Length: 3893, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamreviews[\"total_reviews\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No user reviews            37626\n",
       "Very Positive               9557\n",
       "Mixed                       8970\n",
       "Positive                    8877\n",
       "1 user reviews              7625\n",
       "Mostly Positive             6455\n",
       "2 user reviews              5171\n",
       "3 user reviews              3727\n",
       "4 user reviews              3006\n",
       "5 user reviews              2464\n",
       "6 user reviews              1887\n",
       "7 user reviews              1723\n",
       "Mostly Negative             1505\n",
       "8 user reviews              1464\n",
       "9 user reviews              1238\n",
       "Overwhelmingly Positive      784\n",
       "Negative                     256\n",
       "Very Negative                 52\n",
       "Overwhelmingly Negative        9\n",
       "Name: review_score_desc, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamreviews[\"review_score_desc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    65931\n",
       "8.0     9557\n",
       "5.0     8970\n",
       "7.0     8877\n",
       "6.0     6455\n",
       "4.0     1505\n",
       "9.0      784\n",
       "3.0      256\n",
       "2.0       52\n",
       "1.0        9\n",
       "Name: review_score, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamreviews[\"review_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for duplicates\n",
    "steamreviews.duplicated(subset=\"appid\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up duplicates\n",
    "steamreviews = steamreviews.drop_duplicates(subset=\"appid\", keep=\"last\")\n",
    "steamreviews.to_csv(\"../data/download/steamreviews_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very good. Review Score Description actually gives us more information than the Score alone... although it lumps together all games with less than 10 reviews as a score of 0.\n",
    "\n",
    "We might want to keep only the total reviews as popularity, and feature a column to have a score. But it the categories already stablished in Steam seem to be adequate. In any case, a continuous score is also good, so let's use the one stablished at steam DB: https://steamdb.info/blog/steamdb-rating/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling data colection errors table\n",
    "\n",
    "Keeping track on what apps are missing/removed from the dataset is quite helpful and is necessary for the proper statistical analysis. So here we'll compile the dataset with the missing apps and the rough reasons of why they are missing. It will be used during cleanup as well.\n",
    "\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Mike\\work\\coding\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Loading data tables\n",
    "steam_app_data = pd.read_csv(\"../data/download/steam_app_data.csv\")\n",
    "steam_spy_data = pd.read_csv(\"../data/download/steamspy_data.csv\")\n",
    "steamreviews = pd.read_csv(\"../data/download/steamreviews_data.csv\")\n",
    "\n",
    "steam_app_data = steam_app_data.set_index(\"steam_appid\")\n",
    "steam_spy_data = steam_spy_data.set_index(\"appid\")\n",
    "steamreviews = steamreviews.set_index(\"appid\")\n",
    "\n",
    "#Loading error tables\n",
    "steam_app_errors = pd.read_csv(\"../data/download/steam_errors.csv\")\n",
    "steam_spy_errors = pd.read_csv(\"../data/download/steamspy_errors.csv\")\n",
    "steam_reviews_errors = pd.read_csv(\"../data/download/steamreviews_errors.csv\")\n",
    "\n",
    "#Loading full ids table\n",
    "full_ids = pd.read_csv(\"../data/download/full_steam_ids.csv\")\n",
    "missing_collection = full_ids[~full_ids[\"appid\"].isin(steam_app_data.index) & ~full_ids.index.isin(missing_ids.index)].copy()\n",
    "missing_ids = pd.concat([steam_app_errors,steam_spy_errors,steam_reviews_errors,missing_collection]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_collection = full_ids[~full_ids[\"appid\"].isin(steam_app_data.index) & ~full_ids.index.isin(missing_ids.index)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to redownload the missing ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at index 0:\n",
      "\n",
      "Current index: 3\n",
      "Error getting data for 1061400 with exception Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Current index: 11\n",
      "Error getting data for 1444140 with exception Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Current index: 17\n",
      "Error getting data for 1863540 with exception Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Exported lines 0-99 to steam_app_data_delta.csv. Batch 0 time: 0:03:18 (avg: 0:03:18, remaining: 0:23:05)\n",
      "Exported lines 100-199 to steam_app_data_delta.csv. Batch 1 time: 0:03:16 (avg: 0:03:17, remaining: 0:19:40)\n",
      "Exported lines 200-299 to steam_app_data_delta.csv. Batch 2 time: 0:03:16 (avg: 0:03:17, remaining: 0:16:23)\n",
      "Exported lines 300-399 to steam_app_data_delta.csv. Batch 3 time: 0:03:15 (avg: 0:03:16, remaining: 0:13:05)\n",
      "Exported lines 400-499 to steam_app_data_delta.csv. Batch 4 time: 0:03:15 (avg: 0:03:16, remaining: 0:09:48)\n",
      "Exported lines 500-599 to steam_app_data_delta.csv. Batch 5 time: 0:03:16 (avg: 0:03:16, remaining: 0:06:32)\n",
      "Exported lines 600-699 to steam_app_data_delta.csv. Batch 6 time: 0:03:15 (avg: 0:03:16, remaining: 0:03:16)\n",
      "Exported lines 700-777 to steam_app_data_delta.csv. Batch 7 time: 0:02:37 (avg: 0:03:11, remaining: 0:00:00)\n",
      "\n",
      "Processing batches complete. 778 apps written\n",
      "Starting at index 0:\n",
      "\n",
      "Exported lines 0-99 to steamspy_data.csv. Batch 0 time: 0:02:32 (avg: 0:02:32, remaining: 0:17:44)\n",
      "Exported lines 100-199 to steamspy_data.csv. Batch 1 time: 0:02:31 (avg: 0:02:32, remaining: 0:15:10)\n",
      "Exported lines 200-299 to steamspy_data.csv. Batch 2 time: 0:02:32 (avg: 0:02:32, remaining: 0:12:39)\n",
      "Exported lines 300-399 to steamspy_data.csv. Batch 3 time: 0:02:32 (avg: 0:02:32, remaining: 0:10:07)\n",
      "Exported lines 400-499 to steamspy_data.csv. Batch 4 time: 0:02:31 (avg: 0:02:32, remaining: 0:07:35)\n",
      "Exported lines 500-599 to steamspy_data.csv. Batch 5 time: 0:02:32 (avg: 0:02:32, remaining: 0:05:03)\n",
      "Exported lines 600-699 to steamspy_data.csv. Batch 6 time: 0:02:31 (avg: 0:02:32, remaining: 0:02:32)\n",
      "Exported lines 700-777 to steamspy_data.csv. Batch 7 time: 0:01:58 (avg: 0:02:27, remaining: 0:00:00)\n",
      "\n",
      "Processing batches complete. 778 apps written\n",
      "Starting at index 0:\n",
      "\n",
      "Exported lines 0-99 to steamreviews_data.csv. Batch 0 time: 0:02:47 (avg: 0:02:47, remaining: 0:19:29)\n",
      "Exported lines 100-199 to steamreviews_data.csv. Batch 1 time: 0:02:45 (avg: 0:02:46, remaining: 0:16:35)\n",
      "Exported lines 200-299 to steamreviews_data.csv. Batch 2 time: 0:02:48 (avg: 0:02:46, remaining: 0:13:52)\n",
      "Exported lines 300-399 to steamreviews_data.csv. Batch 3 time: 0:02:49 (avg: 0:02:47, remaining: 0:11:08)\n",
      "Exported lines 400-499 to steamreviews_data.csv. Batch 4 time: 0:02:48 (avg: 0:02:47, remaining: 0:08:22)\n",
      "Exported lines 500-599 to steamreviews_data.csv. Batch 5 time: 0:02:48 (avg: 0:02:47, remaining: 0:05:35)\n",
      "Exported lines 600-699 to steamreviews_data.csv. Batch 6 time: 0:02:48 (avg: 0:02:47, remaining: 0:02:47)\n",
      "Exported lines 700-777 to steamreviews_data.csv. Batch 7 time: 0:02:10 (avg: 0:02:43, remaining: 0:00:00)\n",
      "\n",
      "Processing batches complete. 778 apps written\n"
     ]
    }
   ],
   "source": [
    "steam_errors = []\n",
    "steamspy_errors = []\n",
    "steamreviews_errors = []\n",
    "\n",
    "print(\"Attempting to redownload Storefront data\")\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=missing_ids[\"appid\"],\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data_data,\n",
    "    index_filename=steam_index,\n",
    "    errors_list=steam_errors,\n",
    "    columns=steam_columns,\n",
    "    begin=0,\n",
    "    end=len(missing_ids),\n",
    "    batchsize=100,\n",
    "    pause=1\n",
    ")\n",
    "\n",
    "print(\"Attempting to redownload SteamSpy data\")\n",
    "process_batches(\n",
    "    parser=parse_steamspy_request,\n",
    "    app_list=missing_ids[\"appid\"],\n",
    "    download_path=download_path, \n",
    "    data_filename=steamspy_data,\n",
    "    index_filename=steamspy_index,\n",
    "    errors_list=steamspy_errors,\n",
    "    columns=steamspy_columns,\n",
    "    begin=0,\n",
    "    end=len(missing_ids),\n",
    "    batchsize=100,\n",
    "    pause=1\n",
    ")\n",
    "\n",
    "print(\"Attempting to redownload Steam Review data\")\n",
    "process_batches(\n",
    "    parser=parse_steamreviews_request,\n",
    "    app_list=missing_ids[\"appid\"],\n",
    "    download_path=download_path, \n",
    "    data_filename=steamreviews_data,\n",
    "    index_filename=steamreviews_index,\n",
    "    errors_list=steamreviews_errors,\n",
    "    columns=steamreviews_columns,\n",
    "    begin=0,\n",
    "    end=len(missing_ids),\n",
    "    batchsize=100,\n",
    "    pause=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "steam_app_data = pd.read_csv(\"../data/download/steam_app_data.csv\")\n",
    "steam_app_data = steam_app_data.drop_duplicates(subset=\"steam_appid\", keep=\"last\")\n",
    "steam_app_data.to_csv(\"../data/download/steam_app_data.csv\", index=False)\n",
    "\n",
    "steam_spy_data = pd.read_csv(\"../data/download/steamspy_data.csv\")\n",
    "steam_spy_data = steam_spy_data.drop_duplicates(subset=\"appid\", keep=\"last\")\n",
    "steam_spy_data.to_csv(\"../data/download/steamspy_data.csv\", index=False)\n",
    "\n",
    "steamreviews = pd.read_csv(\"../data/download/steamreviews_data.csv\")\n",
    "steamreviews = steamreviews.drop_duplicates(subset=\"appid\", keep=\"last\")\n",
    "steamreviews.to_csv(\"../data/download/steamreviews_data.csv\", index=False)\n",
    "\n",
    "steam_app_data = steam_app_data.set_index(\"steam_appid\")\n",
    "steam_spy_data = steam_spy_data.set_index(\"appid\")\n",
    "steamreviews = steamreviews.set_index(\"appid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the final missing_ids table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_errors =  pd.DataFrame(steam_errors, columns =['appid'])\n",
    "steam_app_errors.to_csv(\"../data/download/steam_errors.csv\", index=False)\n",
    "\n",
    "steam_spy_errors =  pd.DataFrame(steamspy_errors, columns =['appid'])\n",
    "steam_spy_errors.to_csv(\"../data/download/steamspy_errors.csv\", index=False)\n",
    "\n",
    "steam_reviews_errors = pd.DataFrame(steamreviews_errors, columns =['appid'])\n",
    "steam_reviews_errors.to_csv(\"../data/download/steamreviews_errors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating error table based on steam_app_errors table\n",
    "missing_ids = steam_app_errors.copy()\n",
    "missing_ids[\"reason\"] = \"Steam Download Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding SteamSpy download errors (checking if they are already present)\n",
    "steam_spy_errors[\"reason\"] = \"SteamSpy Download Error\"\n",
    "#Adding Steam Reviews errors\n",
    "steam_reviews_errors[\"reason\"] = \"Steam Review Download Error\"\n",
    "#Adding the missing ids to the error list\n",
    "missing_ids = pd.concat([missing_ids,steam_spy_errors,steam_reviews_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of the missing ids by comparing full_ids witht steam_app data\n",
    "#df.loc[~df.index.isin(df.merge(df2.assign(a='key'),how='left').dropna().index)]\n",
    "missing_collection = full_ids[~full_ids[\"appid\"].isin(steam_app_data.index) & ~full_ids.index.isin(missing_ids.index)].copy()\n",
    "missing_collection[\"reason\"] = \"Steam Storefront Error\"\n",
    "missing_ids = pd.concat([missing_ids,missing_collection]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving resulting missing ids dataframe to csv\n",
    "missing_ids.to_csv(\"../data/download/missing_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the data tables and making steamreviews and steam_spy_data to be consistent with steam_app_data:\n",
    "index_missing_reviews = steam_app_data.index.difference(steamreviews.index)\n",
    "index_missing_steamspy = steam_app_data.index.difference(steam_spy_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JlNtC4nXTI9"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "Here we have defined and demonstrated the download process used to generate the data sets. This is similar to what Nik Davis did in the past, with the exception that now the process can be reinitiated to get only the new IDs in the full id list and add them to the previous dataset. This might be expanded to get also the IDs with data updated.\n",
    "\n",
    "We have two tables now with a lot of information from the apps on Stem Store. From the Steam Store API we have a lot of metadata, which is used by the Steam Store itself to display the Store page. We will consider this the main source of information. The most useful but missing information is the quantity of positive or negative recommendations, we only have the total. Also, the tags (possibly as they were added later) are not available. There might be available in a separate request which is not public, or Valve just forgot to add it to the list.\n",
    "\n",
    "From Steam Spy we have some additional information as it tries to track the concurrent users, we have averages , top... It also offers an estimate of owners, with a very large margin of error. We will check exactly what to keep and how to clean it in the next section.\n",
    "\n",
    "After reviewing the reviews (positive/negative reviews) vs total reviews from Steam Spy and Steam Store in the cleaning section, we discovered that they did not agree so we got a third dataset - review metadata from the Steam Store API (partners). We could have changed the request and instead get all individual reviews, which could be an interesting machine learning analysis - we have data about if they are positive or negative, and we could do a sentiment analysis from the text using NLP. But that is not the focus of our current analysis. Also we have to note that getting the metadata was about 5h, but getting all individual reviews could take quite some time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "1-data-collection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
